# Home_Sales

1. Read the home_sales_revised.csv data in the starter code into a Spark DataFrame.
2. Created a temporary table called home_sales.
3. Import the necessary PySpark SQL functions for this assignment.
4. Created a temporary table called home_sales.
5. Cache your temporary table home_sales.
6. Using the cached data, run the last query that calculates the average price of a home per "view" rating having an average home price greater than or equal to $350,000. Determine the runtime and compare it to uncached runtime.
7. Partition by the "date_built" field on the formatted parquet home sales data.
8. Created a temporary table for the parquet data.
9. Run the last query that calculates the average price of a home per "view" rating having an average home price greater than or equal to $350,000. Determine the runtime and compare it to uncached runtime.
10. Uncache the home_sales temporary table.
11. Verifyed that the home_sales temporary table is uncached using PySpark.
12. Downloaded  Home_Sales.ipynb file and upload it into your "Home_Sales" GitHub repository.
 
